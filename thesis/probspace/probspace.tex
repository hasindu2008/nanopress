\chapter{Problem Space} \label{chap:probspace}

%TODO why lossless important
%TODO why not lossy
%TODO why per read

It is important for fast parallel querying that the reads are compressed separately. That's not to say that each read is independent. Reads originating from the same channel and well/pore will have been recorded by the same sensor and so will likely have similar properties. This is especially true for reads recorded at a similar timestamp as each channel deteriorates over the course of a sequencing run. A compression method may take advantage of these available patterns but it is important for querying that each read can be decompressed independently. Perhaps there exists a multi-read compression strategy which is great for space reduction and hence archival purposes. However, such a strategy is not obvious and the primary desire of bioinformaticians is a better compression algorithm that does not sacrifice parallel querying. Hence, multi-read compression is outside the scope of this thesis.

Lossy compression, although an interesting avenue, is not desirable for most research applications where accuracy is more valued than storage space. For example, long-term archival of scholarly research data must be lossless to allow for public replication of any results. For this reason, lossless compression is the focus of this thesis.

% Explore integer methods without generic

Instead, each integer can be represented more space-efficiently by using $b<16$ bits where \[b(x)=\lceil\log_2(\max(x))\rceil.\] In this case, the compression ratio would be approximately $16/b$:
\begin{align*}
	\text{Compression Ratio} &= \frac{\text{Uncompressed Bytes}}{\text{Compressed Bytes}}\\
	&=\frac{\frac{16}{8}n}{\lceil\frac{b}{8}n\rceil}\\
	&\stackrel{n\to\infty}{\longrightarrow}\frac{16}{b}.
\end{align*}
A better strategy would be to initially transform the data such that its range is decreased.
%Let \[T=\{t\mid t:X\to Y\}\] be the set of bijections from $X$, the set of reads, to $Y := \{(y_i\mid y_i\in \mathbb{N}_0)\}$. One such transformation is defined by
Let \[T=\{t\mid t:X\to X\}\] be the set of functions from $X$, the set of reads, to itself. One such transformation subtracts the minimum of $x$ from each integer and is defined by \[ submin(x) := (x_i-\min(x)). \] Each transformed integer can then be represented using fewer than or the same number of bits:
%A better strategy would be to first transform $x$ by subtracting the minimum of $x$ from each integer. That is, $x\mapsto(x_i-\min(x))$. Then,
\begin{align*}
	b(submin(x))&=\lceil\log_2(\max(submin(x)))\rceil\\
	&=\lceil\log_2(\max(x-\min(x)))\rceil\\
	&=\lceil\log_2(\max(x)-\min(x))\rceil\\
	&\le b(x)
\end{align*}
since $\log$ is an increasing function.
%However, in practice each integer lies within $[2^7,2^{11}]$. This means each integer can be represented using 11 bits rather than 16. If each integer is stored using $b$ bits, the compression ratio would be approximately $16/b$ as follows:
%Hence, using 11 bits per integer results in a compression ratio of approximately $1.\overline{45}$. Note that the optimal $b$ is given by
Another transformation takes the differences between successive signals and is defined by
\[ \delta(x):=(x_{i+1}-x_i\mid 0\le i\le n-2).\]
% TODO analyse delta

\section{Subsequence Searching}
Some of the previously discussed compression methods, such as bit packing and FOR, depend on global statistics of the data such as the minimum and maximum.
For nanopore signal data, these statistics are easily dominated by outliers in the data.
One naive solution is to separately compress adjacent subsequences of equal length.
This approach has previously been successful in the literature with methods such as SIMD-BP128 and fast patched frame-of-reference (FastPFOR) performing compression in blocks of 128 integers \cite{lemire-simd}.

% Explore blocks of different sizes
% Explore blocks of different equal splits

Consider partitioning the nanopore signal into adjacent variable length blocks.
Let $P(x,s)$ be the partitioning of signal $x$ into $|s|=m\ge 1$ partitions according to partitioner $s$ where
\[ s := (s_j \in \mathbb{Z}\cap [0, n) \mid s_0 = 0)\]
and $s_j$ is the starting index of the $j$-th partition such that
\[ P(x,s) = (p_j) := ((x_{s_j},x_{s_j+1},\dots,x_{s_{j+1}-1}))_{j\in\mathbb{Z}\cap[0,m)}.\]
% TODO this is not correct for the last partition
%((x_{s_0},x_{s_0+1},\dots,x_{s_1-1}),(x_{s_1},\dots,x_{s_2-1}),\dots,(x_{s_{m-1}},\dots,x_n)) .\]
For example, if $x=(656,527,515,527,526)$ and $s = (0,3,4)$ then $m=3$ and
\[P(x,s)=((656,527,515),(527),(526)).\]

Given the partitioning $P$ we would like to compress each partition $p_j$ separately and concatenate the results.
Let $M(p)$ be the compressed bytes of partition $p$ after applying compression method $M$.
Then, the compressed bytes of signal $x$ under partitioner $s$ is the concatenation of the compressed bytes of each partition $M(p_j)$ given by
\[ C(x,s,M) := (M(p_j)\mid p_j=P(x,s)_j). \]
Let $\hat s$ be the partitioner of $x$ which minimises the number of compressed bytes
\[ |C(x,\hat s,M)| = \min_s |C(x,s,M)| = \min_s \sum_j|M(p_j)|. \]
The minimum number of compressed bytes can be found using the following recursive relationship
\begin{align*}
	|C(x,\hat s,M)| &= M(x) & n = 1\\
	|C(x,\hat s,M)| &= \min\{M(x),\min_{0\le k\le n-2}\{|C(x_L,\hat s_{x_L},M)| + |C(x_R,\hat s_{x_R},M)|\}\} & n\ge 2
\end{align*}
where $x_L=(x_0,x_1,\dots,x_k)$ and $x_R=(x_{k+1},\dots,x_{n-1})$.
That is, the compressed bytes with minimum length are found by either compressing the whole signal as usual or by dividing the signal into two partitions and concatenating each partition's minimum length compressed bytes.

For a signal of size $n$, the number of comparisons is given by the recurrence relation
\begin{align*}
	c_1 &= c_{M,1}\\
	c_n &= c_{M,n} + \sum_{k=0}^{n-2}(c_{k+1}+c_{n-k-1} + 1) &n\ge 2
	%TODO +1 for adding?
\end{align*}
where $c_{M,n}$ is the number of comparisons for the compression method $M$ on input size $n$.
This can be simplified as follows
\begin{align*}
	c_n &= c_{M,n} + \sum_{k=1}^{n-1}(2c_k + 1)\\
	&= c_{M,n} + n-1 + 2\sum_{k=1}^{n-1}c_k &n\ge 2
\end{align*}
Let's write $c_n$ in terms of $c_{n-1}$ in order to solve the recurrence more easily.
\begin{align*}
	c_{n-1} &= c_{M,n-1} + n - 2 + 2\sum_{k=1}^{n-2}c_k & n\ge 3\\
	c_{n} - c_{n-1} &= c_{M,n} - c_{M,n-1} + 1 + 2c_{n-1} \\
	c_{n} &= c_{M,n} - c_{M,n-1} + 1 + 3c_{n-1} & n\ge 2
\end{align*}
%TODO cite formula here?
Unrolling this we find that
\begin{align*}
	c_n &= c_{M,n} + \sum_{k=1}^{n-1}3^{k-1}(2c_{M,n-k} + 1) & n\ge 2.
\end{align*}
If the compression method has linear time complexity (i.e. $c_{M,n} = O(n)$) then
\begin{align*}
	c_n &= O(n) + \sum_{k=1}^{n-1}3^{k-1}(2O(n-k) + 1)\\
	&= O(n) + \sum_{k=1}^{n-1}3^{k-1}O(n-k)\\
	&= O(3^n) & \text{See Appendix \ref{app:cn}}
\end{align*}
That is, it takes exponential time to find the minimum compressed size using the naive recursive algorithm.
%TODO why different to subsequence sum

However, this algorithm re-computes the compressed bytes of subsequences many times rather than calculating it once and caching the data.
A better approach is to use bottom-up dynamic programming which avoids recursion and hence stack overflows.
The approach would compute the minimum number of compressed bytes for all subsequences of length $1$ then $2$, $3$ and so forth up to $n$. The results are stored in a triangular matrix $OPT\in \mathbb{N}\times\mathbb{N}$ where $OPT_{i,j}$ is the minimum number of compressed bytes of the subsequence starting at index $i$ and ending at index $j$.
It is triangular since we are not interested in subsequences which end before they begin. That is, we require that $i\le j$.

%TODO put algorithm here

For a signal of size $n$, the algorithm has the following number of comparisons
\[ \tilde c_n = \sum_{k=1}^n(n-k+1)(c_{M,k}+k-1) \]
since there are $n-k+1$ subsequences of size $k$ and each must compare compressing itself to concatenating $k-1$ pairs of compressed subdivisions.
Suppose as before that the compression method takes linear time. Then $\tilde c_n$ simplifies to
\begin{align*}
	\tilde c_n &= \sum_{k=1}^n(n-k+1)(O(k)+k-1)\\
	&= \sum_{k=1}^n(n-k+1)O(k)\\
	&= O(n^3) &\text{See Appendix \ref{app:cn-dyn}}
\end{align*}
Interestingly, $c_n$ and $\tilde c_n$ are asymptotically related by having opposite base and power terms; 3 and $n$.

%What is the length of these subsequences?

We can speed up the algorithm without a significant loss in compression by ignoring subsequences smaller than $l$ and moving between subsequence lengths at a step of $\delta_1$ and subsequences of the same length at a step of $\delta_2$.
The idea is that for compression method $M$, there exists some $l$ such that the minimum number of compressed bytes of subsequence $y$ with size $|y|<l$ equals the number of regular compressed bytes of $y$ with high probability.
In mathematical notation $\exists l\in\mathbb{N}^+$ such that $\forall x,\;|x|<l$
\[ P(C(x,\hat s,M)=M(x))> 1-\epsilon\]
for small $\epsilon \le 0.05$.
Furthermore, it is not necessary to calculate subsequences of all lengths or all subsequences of the same length since subsequences which differ by a couple signals should not provide a significantly better partition. This is because the deltas between successive signal values are small.
Rather, we want to calculate the minimum number of compressed bytes for subsequences of length $l,l+\delta_1,l+2\delta_1,\dots,l + \lfloor\frac{n-l}{\delta_1}\rfloor\delta_1, n$.
Then for subsequences of a particular length $p$ we calculate the minimum number of compressed bytes for subsequences $\delta_2$ apart, of the form $(x_{i\delta_2},x_{1+i\delta_2},\dots,x_{p-1+ i\delta_2})$ where $0\le i\le \lfloor \frac{n-p}{\delta_2} \rfloor$.

%TODO put algorithm here

For a signal of size $n$, the algorithm now has the following number of comparisons
%\[ \tilde c_n = \sum_{k=l}^n(n-k+1)(c_{M,k}+k-1) \]

However, we can exploit the characteristics of nanopore signal data to find better subsequences.
For example, each read typically consists of a stall.

%The next idea is to divide $x$ into subsequences with small range and compress each subsequence separately.
%Let $OPT(i, j)$ be the minimum number of bytes after compressing $(x_i,\dots, x_j)$. Then,
%\[ OPT(i, j) = \min(bytes(i, j),\min_{i\le k<j}(OPT(i, k) + OPT(k+1, j))) \]
