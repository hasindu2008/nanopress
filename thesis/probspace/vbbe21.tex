\section{One Byte, Two Byte Exceptions}
\label{sec:vbbe21}

The one byte, two byte exceptions encoding, abbreviated to \textit{vbe21}, encodes a list of integers using one byte for each integer except for integers which cannot fit into one byte, known as \textit{exceptions}, which are encoded using two bytes. Rather than using a control code to mark where normal data and exceptions occur as in the Stream VByte codec (Section \ref{subsubsec:svb}), the exceptions are encoded at the beginning since it is expected that exceptions will occur with small probability.

The number of exceptions is written using 2 bytes, followed by the exceptions' positions in the list using 4 bytes each and the exceptions themselves using 2 bytes each.

Now, consider applying the zig-zag delta encoding to a read and then compressing using encoding $A$. Let $C_A:\Omega\to\mathbb{N}_0$ be a random variable measuring the resulting compressed size in bytes where $\Omega$ is the space of reads. Then
\begin{align*}
	C_{vbe21} &= 2 + 6X + (N - X)\\
	&= 2 + 5X + N
\end{align*}
where $N$ and $X$ are random variables measuring the read length and number of exceptions respectively.
Recall from Section \ref{subsec:prob} that the probability that a data point is greater than 255 and therefore outside the one byte range is $\sim 5\times 10^{-5}$. Thus, the expected number of exceptions is
\[ E[X] = (5 \times 10^{-5})E[N] \]
%Also, the read lengths can be modelled by the Gamma distribution $\Gamma(1.0885,0.0096)$.
where the expected read length is
\[ E[N] = 113471.4 \]
from Table \ref{tab:n}.
Then, the expected compressed size is given by
\begin{align*}
	E[C_{vbe21}] &= 2 + (2.5\times 10^{-4})E[N]+ E[N]\\
	&= 2 + 1.00025E[N]\\
	&\approx 113502.
\end{align*}
In comparison, using the Stream VByte 16 (or \textit{svb16}) encoding, the compressed size is given by
\begin{align*}
	C_{svb16} &= \lceil N/8 \rceil + 2X + (N - X)\\
	&= \lceil N/8 \rceil + X + N.
\end{align*}
Then, the expected compressed size is
\begin{align*}
	E[C_{svb16}] &= \lceil E[N]/8 \rceil + (5 \times 10^{-5})E[N] + E[N]\\
	&= \lceil E[N]/8 \rceil + 1.00005E[N]\\
	&\approx 127661\\
	&> E[C_{vbe21}].
\end{align*}
So it is clear that \textit{vbe21} saves more space than \textit{svb16}: roughly 14 KiB on average per read or an estimated 6.6 GiB for the whole data set. This is actually a very accurate estimation; see Figure \ref{fig:svb-vbe21-zd-size}.

\input{plots/svb-vbe21-zd-size}

\subsection{Exceptions Encoding}

The number of exceptions per read ranges from 0 to 3616 but 99\% of reads have between 0 and 42 exceptions. The mean is $\sim$5 and the standard deviation is $\sim$18. See Table \ref{tab:ex}. Its distribution, shown in Figure \ref{fig:nex-hist}, could be nicely fitted by an exponential distribution. Notice that around 20\% of the reads have no exceptions such that their zig-zag deltas lie perfectly between 0 and 255.

\input{plots/ex-tab}
\input{plots/nex-hist}

The exceptions range from 256 to 2317 as expected but 99\% of the exceptions do not go higher than 481. The mean and mode are $\sim$277 and 256 respectively. Refer to Table \ref{tab:ex}. The histogram of exceptions is shown in Figure \ref{fig:ex-hist}. Notably, the frequency of even-numbered exceptions is much greater than adjacent odd-numbered exceptions from 256 up to $\sim$330. This is due to large positive deltas occuring more frequently than large negative deltas as previously discussed in Section \ref{subsec:stripe}.

\input{plots/ex-hist}

Given this information, there are better ways of storing the exceptions than in a raw sequential format.

% TODO show these ways
% analyse how much better

However, the exceptions section usually accounts for only 36 bytes of the read on average in the vbe21 format. Hence, focussing on compressing the one-byte data section has much more potential benefit.

\subsection{Huffman}

Consider compressing the one-byte data using the Huffman coding algorithm. This requires determining the read's zig-zag delta distribution on an initial pass. Then, the Huffman table is recorded and each byte is encoded with its Huffman code. The problem is that there is an overhead with storing the table.
Naively, one can store the table by writing the number of entries in the table (1 byte), the table's size in bytes (4 bytes), then each entry's symbol (1 byte), code length (1 byte) and code (code length in bytes). The resulting table consumes
\[ 5 + 2m + \sum_{i=1}^m\lceil b_i / 8 \rceil \]
bytes where $m\in\mathbb{Z}\cap[0,255]$ is the number of entries in the table and $b_i$ is the length of the $i$-th entry's code in bits.

\subsection{Range Coding}
