%\section{The Focus}
% TODO rename this
% Frame as a question: how to compress nanopore data efficiently?
% Quantify the performance in terms of storage and analysis

%TODO why lossless important
%TODO why not lossy
%TODO why per read

\textit{What makes a compression method suitable for nanopore signal data?}

This is a question we must first answer before proceeding with our search for
novel methods.
The region of suitable compression methods is known as the \textit{problem space} and
is the focus of this chapter. But in order to explore the problem space we must
first define it; a task which depends entirely on how one intends to use the
data.

The intention behind most holders of nanopore data is either research or
clinical diagnosis. In both events, the data is usually useless without first
basecalling it, after which it can be analysed using an assortment of downstream
analysis software.
If it is compressed it will need to be decompressed in order to
perform these steps. After analysis the data will typically be put aside for
many years with infrequent or no use due to many legal regulations requiring
clinical and published research data to be archived for several years.

Hence, there are two main use-cases for nanopore data: analysis and archival
which are short- and long-term tasks respectively. In the short-term, a
bioinformatician performing analysis would desire a balance between fast
analysis speed and a high compression ratio. However, in the long-term, an
archivist would desire the highest compression ratio achievable as long as there
is still a practical decompression speed. This provides us with two problem
subspaces to further define.
% Example of long-term archival requirments

Let's define a suitable compression method for nanopore signal data as
one which
\begin{enumerate}
	\item is lossless and
	\item uses less bits per point than the entropy of the data (7.70 bits per point).
\end{enumerate}
This means that a suitable method has a compression ratio of at least $16/7.70 \approx 2.08$.
Moreover, it must be lossless because for most research and all clinical
applications accuracy is essential. For
the analysis subspace let's further define a suitable compression method as one
\begin{enumerate}
	\item[(3)] which can independently decompress each read.
\end{enumerate}
This is necessary because many
reads are typically requested by an analysis program from random locations in
the data. Rather than decompressing the whole data set, it is faster to jump to
the beginning of a requested read and decompress it independently.
Necessarily this means that each read must be compressed separately.
Furthermore, this allows multiple read requests to be handled in parallel by a
multi-core processor which significantly speeds up analysis.

However, beyond the above definitions, the suitability of a compression method
for each subspace is highly subjective, depending on the type of analysis being
performed, the amount of data being archived, its frequency of retrieval, the
level of access to computational resources, etc. Each specific application
demands different requirements from the problem space which are hard to predict.
The best we can do is to generally define an order of suitability for each
subspace by which one compression method can be definitively compared to
another. Given two suitable compression methods $M_1$ and $M_2$, we say that
$M_2$ is more suitable than $M_1$ (i.e. $M_2>_sM_1$) if
\[\Delta(M_1,M_2):=\delta_2-\delta_1 - \alpha(t^c_2-t^c_1) - \beta(t^d_2-t^d_1) > 0\]
and $M_2$ is equally as suitable as $M_1$ (i.e. $M_2\equiv_sM_1$) if
\[\Delta(M_1,M_2)= 0\]
where $\alpha$ and $\beta$ are user-defined constants, $\delta_i$ is the
space saving and $t^c_i,t^d_i$ are the average times to
compress and decompress 1 TiB (in hours) respectively using one thread with
method $M_i$. The space saving which ranges from 0 to 1 is the proportion of
the uncompressed size that was reduced via compression. This model is making a
lot of assumptions about the linearity of the problem space but let's consider
it for simplicity.

Let's define $(\alpha,\beta)$ for each subspace based on the desires of
bioinformaticians working with nanopore signal data. Consider yourself a
bioinformatician intending to analyse a nanopore signal data set which you have
compressed with method $M_1$. You envisage a new method $M_2$ with the same
space saving as $M_1$ ($\delta_2=\delta_1$) but decompression is an hour quicker
per TiB ($t^d_2=t^d_1-1$). What is the maximum extra number of hours per TiB
that you would be willing to spend during compression to use $M_2$ over $M_1$?
The answer depends on the bioinformatician and whether decompression is a
bottleneck during analysis which is often dominated by basecalling. But let's
say you would wait an extra 3 hours per TiB ($t^c_2=t^c_1+3$). Then,
$M_1\equiv_sM_2$ since any more than 3 hours per TiB and you would instead
choose $M_1$. Hence,
\begin{align*}
	\Delta(M_1,M_2)&=0\\
	\delta_2-\delta_1-\alpha(t^c_2-t^c_1) - \beta(t^d_2-t^d_1) &= 0\\
	0-3\alpha + \beta &= 0\\
	\beta &= 3\alpha.
\end{align*}
Now, imagine a different method $M_3$ which
takes the same time during decompression as $M_1$ ($t^d_3=t^d_1$) but you save
10\% more of the original space (i.e. 102.4 GiB for 1 TiB) ($\delta_3=\delta_1+0.1$).
Again, I ask:
What is the maximum extra number of hours per TiB that you would be willing to
spend during compression to use $M_3$ over $M_1$? This is a significant space
saving
and depending on the amount of time and storage available the answer could
range from 2 to 20 extra hours per TiB. However, for the average
storage-conscious bioinformatician let's say you would wait up to 10 extra hours
per TiB and no longer ($t^c_3=t^c_1+10$). Then,
\begin{align*}
	\Delta(M_1,M_3)&=0\\
	\delta_3-\delta_1 - \alpha(t^c_3-t^c_1) - \beta(t^d_3-t^d_1) &= 0\\
	0.1 - 10\alpha - 0 &=0\\
	\alpha&=0.01\\
	\implies \beta&=0.03.
\end{align*}
Hence, for the analysis subspace $(\alpha,\beta)=(0.01,0.03)$. For example,
suppose there exists another compression method $M_4$ which takes the same time
during compression as $M_1$ ($t_4^c=t_1^c$) but takes an extra hour per TiB
during decompression ($t_4^d=t_1^d+1$). Then, according to our calculations
\begin{align*}
	\Delta(M_1,M_4)&=0\\
	\delta_4-\delta_1 - 0.01(t^c_4-t^c_1) - 0.03(t^d_4-t^d_1) &= 0\\
	\delta_4-\delta_1 - 0.03 &= 0\\
	\delta_4&=\delta_1 + 0.03.\\
\end{align*}
That is, you would allow up to an extra hour per TiB during decompression for at
least 3\% more of the original space saved (i.e. 30.72 GiB for 1 TiB) if you were
using the data for analysis.

Now, let's apply the same thought experiments to the archival subspace. Recall
that $M_2$ has the same space saving as $M_1$ but decompression is an hour
quicker per TiB. What is the maximum extra number of hours per TiB that you
would be willing to spend during compression to use $M_2$ over $M_1$? Since
compression is only performed once during archiving and decompression may be
performed a handful of times or more, the answer is likely somewhere between 5
and 10 extra hours per TiB depending on the expected frequency of decompression.
But let's say you would wait at most 5 extra hours per TiB. Furthermore, recall
that $M_3$ takes the same time during decompression as $M_1$ but you save
10\% more of the original space. Again: What is the maximum extra number
of hours per TiB that you would be willing to spend during compression to use
$M_3$ over $M_1$? Since storage is the primary concern during archiving, the
time spent is likely up to 96 extra hours per TiB. This may seem like a long
time but remember that this is using only one thread and archives will often
need to be stored untouched for several years. Using these answers we find that
$(\alpha,\beta)=(\frac{1}{960},\frac{1}{192})$ for the archival subspace.

%A compression strategy exists in the analysis problem space if
%on a modern computer
%it takes at most 1 hour per TiB on average for compression, at most 0.5 hours
%per TiB on average for decompression and has a compression ratio of at.
%\begin{itemize}
%\item a compression ratio $\ge$ state-of-the-art compression ratio,
%\item a compression speed $\le$ 60 mins per TiB,
%\item a decompression speed $\le$ 30 mins per TiB, and
%\item can perform independent read decompression.
%\end{itemize}
% TODO ignoring I/O

%These are arbitrarily chosen restrictions based on the desires of researchers at
%the Garvan Institute of Medical Research as well as on the current performance
%of the state-of-the-art method VBZ. In fact, using this data set with at least 2
%threads of execution VBZ performs to the above criteria. See Table
%\ref{tab:vbz}.

%has a reasonable compression ratio and speed, and results in a fast analysis speed.
%Let's define a reasonable compression ratio to be greater than or equal to 2.5. This means that for a 1 TiB data set, the compressed data would consume at most 256 GiB.
%Compression is often performed once so a reasonable compression time for a 1 TiB data set is less than or equal to 1 hour on a modern CPU with at least 4 threads.
%If each data point consumes 2 bytes in the uncompressed raw binary format and the average read length is roughly \num{110000}, there are around 550 billion data points and 5 million reads. This equates to $7.2\times 10^{-4}$ seconds on average per read. A compression strategy impacts the speed of analysis through the speed at which it returns a read's uncompressed data, also known as its \textit{access speed}. Many reads are typically requested by an analysis program from random locations in the data so in order to achieve a fast access speed it is important that read decompression is able to be performed in parallel to take advantage of multi-threaded CPU design. That is, each read must be able to be decompressed independently of one another.
%Necessarily this means that each read must be compressed separately, which is historically how nanopore signal compression has been performed. Then, the major overhead with read access becomes decompression speed.

%Analysis typically consists of querying the compressed data for certain reads combined with performing operations over the received data.

%First and foremost a suitable compression strategy should result in a compression ratio strictly greater than one. In other words, it should actually compress the data.
%The next restriction is that it should be lossless. In most research applications accuracy is of much higher value than storage space.

%This thesis focuses on investigating lossless compression methods which are more space efficient and do not compromise analysis speed.
%Necessarily this means that each read must be compressed separately, which is historically how nanopore signal compression has been performed.

These parameters will be used to justify comparisons made in Chapter
\ref{chap:results}. However, in the meantime let's simply explore the analysis
subspace which is a subset of the archival subspace. That is, methods which
are (1) lossless, (2) use less bits per point that the entropy of the data and
(3) can independently decompress each read. Thus, for the remainder of this
thesis we will be investigating lossless per-read compression methods. The bits
used per point restriction is not so difficult to achieve as we'll discover once
the delta transformation is applied.
