\subsection{Subsequence Searching}
Recall that there are many sections of a read with very different behaviour. See
Section \ref{sec:data:char} for a recap of the different types of sections. The
position and presence of these sections is often predictable. For example, the
stall usually begins within the first 50 data points and is a highly likely
occurrence. However, many sections such as the homopolymer and slow sections
occur unpredictably both in terms of position and frequency. Furthermore,
incongruent sections with uncommon behaviour are difficult to handle.

Hence, one idea is to divide the signal into adjacent blocks which capture these
sections, compress each block separately and then combine the resulting data
into a final compressed stream. The problem then becomes how to divide the
blocks such that the compression ratio is minimised. This problem is formally
defined below.

%to attempt to
%capture these subsequences by finding the optimal set of adjacent subsequences
%for which the total compressed size after compressing each subsequence
%separately is minimised.

%Some of the previously discussed compression methods, such as bit packing and
%FOR, depend on global statistics of the data such as the minimum and maximum.
%For nanopore signal data, these statistics are easily dominated by outliers in
%the data.  One naive solution is to separately compress adjacent subsequences of
%equal length.  This approach has previously been successful in the literature
%with methods such as SIMD-BP128 and fast patched frame-of-reference (FastPFOR)
%performing compression in blocks of 128 integers \cite{lemire-simd}.

% Explore blocks of different sizes
% Explore blocks of different equal splits

Consider partitioning the nanopore signal into adjacent variable length
blocks. Recall that the signal can be represented by
\[ x = (x_0,x_1,\dots,x_{n-1}). \]
Let $s=(s_0,s_1,\dots,s_{m-1}\mid s_i<s_{i+1})$ be the partitioner of $x$ such
that the signal is divided into $m$ blocks with the $j$-th block starting at
index $s_j$ and ending at index $s_{j+1}-1$ except for final block which starts
at index $s_{m-1}$ and ends at index $n-1$. Let $P(x,s)$ be the resulting
partitioning of $x$ according to partioner $s$ defined by
\[ P(x,s) = (p_j) := ((x_{s_j},x_{s_j+1},\dots,x_{s_{j+1}-1}))_{j\in\mathbb{Z}\cap[0,m)}.\]
%Let $P(x,s)$ be the partitioning of signal $x$ into $|s|=m\ge 1$
%partitions according to partitioner $s$ where
%\[ s := (s_j \in \mathbb{Z}\cap [0, n) \mid s_0 = 0)\]
%and $s_j$ is the starting index of the $j$-th partition such that
%\[ P(x,s) = (p_j) := ((x_{s_j},x_{s_j+1},\dots,x_{s_{j+1}-1}))_{j\in\mathbb{Z}\cap[0,m)}.\]
% TODO this is not correct for the last partition
%((x_{s_0},x_{s_0+1},\dots,x_{s_1-1}),(x_{s_1},\dots,x_{s_2-1}),\dots,(x_{s_{m-1}},\dots,x_n)) .\]
For example, if $x=(656,527,515,527,526)$ and the partitioner $s = (0,3,4)$ then
there are $m=3$ blocks beginning at indices 0, 3 and 4 such that
\[P(x,s)=((656,527,515),(527),(526)).\]

Given the partitioning $P$ we would like to compress each block $p_j$
separately and concatenate the results. Let $C_M(p)$ be the compressed bytes of
block $p$ after applying compression method $M$. Then, the compressed bytes
of signal $x$ under partitioner $s$ is the concatenation of the compressed bytes
of each block $C_M(p_j)$ given by
\[ C_M(x,s) := (C_M(p_j)\mid p_j=P(x,s)_j). \]
The compressed size of the partitioning is equal to the sum of each block's compressed size:
\[ |C_M(x,s)| = \sum_j|C_M(p_j)|. \]

Now, our desire is to minimise the compressed size $|C_M(x,s)|$ by choosing the
optimal partitioner $s$. Let $\hat s$ be the partitioner of $x$ which minimises
the number of compressed bytes. That is,
\[ |C_M(x,\hat s)| = \min_s |C_M(x,s)| = \min_s \sum_j|C_M(p_j)|. \]

We can solve this optimisation problem by using the following recursive
relationship
\begin{align*}
	|C_M(x,\hat s)| &= |C_M(x)| & n = 1\\
	|C_M(x,\hat s)| &= \min\{|C_M(x)|,\min_{0\le k\le n-2}\{|C_M(x_L,\hat s_{x_L})| + |C_M(x_R,\hat s_{x_R})|\}\} & n\ge 2
\end{align*}
where $x_L=(x_0,x_1,\dots,x_k)$ and $x_R=(x_{k+1},\dots,x_{n-1})$.
That is, the compressed bytes with minimum length are found by either
compressing the whole signal as usual or by dividing the signal into two blocks
and concatenating each block's optimal compressed bytes. Algorithm \ref{alg:rec}
uses the above relationship to calculate the minimum compressed size.

\input{plots/alg-rec}

Let's analyse the time complexity of this algorithm.
For a signal of size $n$, the number of comparisons is given by the recurrence relation
\begin{align*}
	c_1 &= c_{M,1}\\
	c_n &= c_{M,n} + \sum_{k=0}^{n-2}(c_{k+1}+c_{n-k-1} + 1) &n\ge 2
	%TODO +1 for adding?
\end{align*}
where $c_{M,n}$ is the number of comparisons for the compression method $M$ on input size $n$.
This can be simplified as follows
\begin{align*}
	c_n &= c_{M,n} + \sum_{k=1}^{n-1}(2c_k + 1)\\
	&= c_{M,n} + n-1 + 2\sum_{k=1}^{n-1}c_k &n\ge 2
\end{align*}
Let's write $c_n$ in terms of $c_{n-1}$ in order to solve the recurrence more easily.
\begin{align*}
	c_{n-1} &= c_{M,n-1} + n - 2 + 2\sum_{k=1}^{n-2}c_k & n\ge 3\\
	c_{n} - c_{n-1} &= c_{M,n} - c_{M,n-1} + 1 + 2c_{n-1} \\
	c_{n} &= c_{M,n} - c_{M,n-1} + 1 + 3c_{n-1} & n\ge 2
\end{align*}
%TODO cite formula here?
Unrolling this we find that
\begin{align*}
	c_n &= c_{M,n} + \sum_{k=1}^{n-1}3^{k-1}(2c_{M,n-k} + 1) & n\ge 2.
\end{align*}
If the compression method has linear time complexity (i.e. $c_{M,n} = O(n)$) then
\begin{align*}
	c_n &= O(n) + \sum_{k=1}^{n-1}3^{k-1}(2O(n-k) + 1)\\
	&= O(n) + \sum_{k=1}^{n-1}3^{k-1}O(n-k)\\
	&= O(3^n) & \text{See Appendix \ref{app:cn}}
\end{align*}
That is, it takes exponential time to find the minimum compressed size using
this recursive algorithm given $M$ takes linear time during compression.
% TODO is assumption accurate?
%This assumption is not always 
%TODO why different to subsequence sum

However, this algorithm naively re-computes the compressed bytes of each block many
times rather than calculating it once and caching the data. A better approach
is to use bottom-up dynamic programming which avoids recursion and
hence stack overflows in implementation.
The approach would compute the minimum number of compressed bytes for all blocks
of length $1$ then $2$, $3$ and so forth up to $n$. The results are stored in a
triangular matrix $OPT\in \mathbb{N}\times\mathbb{N}$ where $OPT_{i,j}$ is the
minimum number of compressed bytes of the block starting at index $i$ and
ending at index $j$. It is triangular since we are not interested in blocks
which end before they begin. That is, we require that $i\le j$. Algorithm
\ref{alg:dyn} shows the pseudocode for this algorithm.

\input{plots/alg-dyn}

For a signal of size $n$, the algorithm has the following number of comparisons
\[ \tilde c_n = \sum_{k=1}^n(n-k+1)(c_{M,k}+k-1) \]
since there are $n-k+1$ blocks of size $k$ and each must compare compressing
itself to concatenating $k-1$ pairs of compressed subdivisions.
Suppose as before that the compression method takes linear time. Then $\tilde c_n$ simplifies to
\begin{align*}
	\tilde c_n &= \sum_{k=1}^n(n-k+1)(O(k)+k-1)\\
	&= \sum_{k=1}^n(n-k+1)O(k)\\
	&= O(n^3) &\text{See Appendix \ref{app:cn-dyn}}
\end{align*}
%Interestingly, $c_n$ and $\tilde c_n$ are asymptotically related by having opposite base and power terms; 3 and $n$.
This is cubic in time complexity which is an improvement from exponential time
but at the cost of storing $OPT$ which uses $O(n^2)$ space.

%What is the length of these subsequences?

We can speed up the algorithm without a significant loss in compression by
ignoring blocks smaller than $k'$ and moving between block lengths and blocks of
the same length at a step of $\delta$ where $k'$ is divisible by $\delta$ and
$k'>\delta$.
The idea is that for compression method $M$, there should exist some block size
$k'$ for which all blocks smaller than $k'$ are minimally compressed without
partitioning with high probability. Whilst all blocks of size greater than or
equal to $k'$ have a higher probability of being compressed smaller by
partitioning.
%That is, there exists some $k'$ such that the
%minimum number of compressed bytes of block $y$ with size $|y|<k'$ equals
%the number of regular compressed bytes of $y$ with high probability.
That is, $\exists k'\in\mathbb{N}^+$ such that for block $y$
\[ P(C_M(y,\hat s)=C_M(y)\mid |y|<k')> 1-\epsilon\]
and
\[ P(C_M(y,\hat s)=C_M(y)\mid |y|\ge k')\le 1-\epsilon\]
with small $\epsilon \le 0.05$.
Furthermore, it is not necessary to calculate blocks of all lengths or all
blocks of the same length since blocks which differ by a couple signals should
not provide a significantly better partition.
%This is because the deltas between successive signal values are small.
Rather, we want to calculate the minimum
number of compressed bytes for blocks of length
$k',k'+\delta,k'+2\delta,\dots,k' + \lfloor\frac{n-k'}{\delta}\rfloor\delta$.
Then for blocks of a particular length $k$ we calculate the minimum number of
compressed bytes for blocks $\delta$ apart, of the form
$(x_{b\delta},x_{b\delta+1},\dots,x_{b\delta+k-1})$ where $0\le b\le \lfloor \frac{n-k}{\delta} \rfloor$.
Furthermore, for the final block of length $n$ we simply take the partitioning
of the largest block of length $\lfloor \frac{n-k'}{\delta}\rfloor\delta$ which
may or may not equal $n$. See Algorithm \ref{alg:dyn-approx} for the pseudocode
of this approximation algorithm.

\input{plots/alg-dyn-approx}

This should not improve the asymptotic time complexity of the algorithm but
significantly decrease its running time in practice by a factor of $\delta$ or
even greater.
This is because by stepping to the next block or block size by $\delta$, the
algorithm is effectively addressing only $1/\delta$ of the problem.

These optimal-seeking algorithms have a compression time complexity which is
unfavourable in practice, especially considering the size of each read which can
range up to roughly 6 million points.
% TODO approximate running time
% TODO is O(n) actually not true in practice
Furthermore, the cubic running time of the dynamic programming algorithm makes
the poor assumption that the time complexity of the base compression algorithm
is linear. This is not always true or desirable.
It also requires quadratic space to maintain the minimum compressed size
calculations. The space required is further increased when modifying the
algorithm to actually compress the input since
it must maintain the compressed data of each block or instead the optimal
partitioning of each subsequence. Decompression of the blocks
should not be time consuming and can be done in $O(mT)$ time where $m$ is the
number of blocks and $T$ is the time complexity of the base compression
algorithm during decompression. We can likely treat $m$ to be some function of
$n$ so this is polynomial is the best of cases but a lot faster than compression.

For all the above reasons, a faster approximation of the optimal blocks is
desired to improve the compression ratio without significantly impacting the speed of
compression.
